{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist data readed.\n",
      "Train set size:60000, Test set size:10000\n",
      "Data label:6\n",
      "Data size:784\n",
      "Data:\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  92 253 192\n",
      "  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 169 252 253 181  19   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      " 252 253 233  43   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   7 178 252 240  71   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  57 252 252 140   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 197 252 252  94   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  67 240 252 252   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  85 252 252 236   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  85 252 252 189   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  86 253 253 253 255 206  88   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  85 252 252\n",
      " 252 253 252 246 209  38   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  26 200 252 252 252 253 252 252 252 221   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 113 252\n",
      " 252 252 252 253 252 252 252 252  95   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 113 252 252 252 252 112 158 252 252 252\n",
      " 140   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  29 253\n",
      " 255 253 253 253 253  63  85 253 253 253 141   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  29 252 253 252 252 252 252 241 155 252\n",
      " 252 252  94   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  22 227 253 252 252 252 252 253 252 252 245 195   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  47 237 252 252 252 252 253\n",
      " 252 252 208   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  50 112 112 112 112 253 252 141  37   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "\n",
    "\n",
    "#setting logger---------------------------------------\n",
    "def init_logger():\n",
    "    logger = logging.getLogger('NNET Debug')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    # 再创建一个handler，用于输出到控制台\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    # 定义handler的输出格式\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s \\n %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(ch)\n",
    "    return logger\n",
    "#handling data---------------------------------------\n",
    "class StructData:\n",
    "    def __init__(self,data_set):\n",
    "        self.size = len(data_set[0])\n",
    "        self.data = np.array(data_set[0])\n",
    "        self.label = np.array(data_set[1])\n",
    "        self.curr_batch_start = 0;\n",
    "        \n",
    "    def shuffle(self):\n",
    "        append_list = np.append(self.data, self.label.reshape(self.size,1), axis = 1)\n",
    "        np.random.shuffle(append_list)\n",
    "        self.data = append_list[:,:-1]\n",
    "        self.label = append_list[:,-1]\n",
    "    #act like a circular list\n",
    "    def get_next_batch(self,batch_size):\n",
    "        start = self.curr_batch_start\n",
    "        end = (self.curr_batch_start + batch_size) % self.size \n",
    "        self.curr_batch_start = end\n",
    "\n",
    "        if(start < end):\n",
    "            return self.data[start:end],self.label[start:end]\n",
    "        else:\n",
    "            data = np.append(self.data[start:self.size], self.data[0:end], axis = 0)\n",
    "            label = np.append(self.label[start:self.size], self.label[0:end], axis = 0)\n",
    "            return data,label\n",
    "#reading data---------------------------------------        \n",
    "mndata = MNIST('mnist_data')\n",
    "mnist_train=StructData(mndata.load_training())\n",
    "mnist_train.shuffle()\n",
    "mnist_test=StructData(mndata.load_testing())\n",
    "#---------------------------------------\n",
    "'''\n",
    "logger = init_logger()\n",
    "logger.info('Mnist data readed.')\n",
    "logger.info('Train set size:{0},Test set size:{1}'.format(len(mnist_train.data),len(mnist_test.data)))\n",
    "logger.debug('Data size:{0}'.format(len(mnist_test.data[0])))\n",
    "logger.debug('Data:\\n{0}'.format(mnist_test.data[0]))\n",
    "logger.debug('Data label:{0}'.format(mnist_test.label[0]))\n",
    "'''\n",
    "#---------------------------------------\n",
    "print('Mnist data readed.')\n",
    "print('Train set size:{0}, Test set size:{1}'.format(len(mnist_train.data),len(mnist_test.data)))\n",
    "print('Data label:{0}'.format(mnist_train.label[0]))\n",
    "print('Data size:{0}'.format(len(mnist_train.data[0])))\n",
    "print('Data:\\n{0}'.format(mnist_train.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network component defining---------------------------------\n",
    "class AffineComponent:\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = 0.01 * np.random.randn(input_dim, output_dim)\n",
    "        self.bias = 0.01 * np.random.randn(1,output_dim)\n",
    "\n",
    "    def propagate(self,input_data):\n",
    "        assert input_data.shape[1] == self.input_dim\n",
    "        self.input_data = input_data\n",
    "        return input_data.dot(self.weights) + self.bias\n",
    "\n",
    "    def back_propagate(self,derivative):\n",
    "        assert derivative.shape[1] == self.output_dim\n",
    "        propagate_derivative = derivative.dot(self.weights.T)\n",
    "        self.__update(derivative)\n",
    "        return propagate_derivative\n",
    "\n",
    "    def __update(self,derivative):\n",
    "        #the learning rate is a global value which will change while training\n",
    "        #see more detain in LearningRateScheduler\n",
    "        self.bias -= learning_rate * derivative.sum(axis = 0, keepdims= True )\n",
    "        self.weights -= learning_rate * self.input_data.T.dot(derivative)\n",
    "\n",
    "\n",
    "class NolinearComponent:\n",
    "    def __init__(self,dim,nolinear_type):\n",
    "        self.dim = dim\n",
    "        self.nolinear_type = nolinear_type\n",
    "\n",
    "    def propagate(self,input_data):\n",
    "        assert input_data.shape[1] == self.dim\n",
    "        self.input_data = input_data\n",
    "        if(self.nolinear_type == \"relu\"):\n",
    "            return self.__relu(input_data)\n",
    "        else:\n",
    "            #program is not expected to reach here\n",
    "            assert false\n",
    "\n",
    "    def __relu(self,input_data):\n",
    "        #important! must use copy or the input data will be change through index\n",
    "        output_data = input_data.copy()\n",
    "        output_data[output_data < 0] = 0\n",
    "        return output_data\n",
    "    #----------------------------\n",
    "    def back_propagate(self,derivative):\n",
    "        assert derivative.shape[1] == self.dim\n",
    "        if(self.nolinear_type == \"relu\"):\n",
    "            return self.__back_relu(derivative)\n",
    "        else:\n",
    "            #program is not expected to reach here\n",
    "            assert false\n",
    "            \n",
    "    def __back_relu(self,derivative):\n",
    "        derivative[self.input_data < 0] = 0\n",
    "        return derivative\n",
    "\n",
    "\n",
    "class SoftmaxOutputComponent:\n",
    "    def __init__(self,dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def propagate(self,input_data):\n",
    "        assert input_data.shape[1] == self.dim\n",
    "        self.input_data = input_data\n",
    "        e_x = np.exp(input_data)\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def back_propagate(self,probs,label):\n",
    "        assert probs.shape[0] == label.shape[0]\n",
    "        batch_size = probs.shape[0]\n",
    "        delta = probs\n",
    "        delta[range(batch_size),batch_label] -= 1\n",
    "        return delta / batch_size\n",
    "\n",
    "'''\n",
    "this class aims to change the learning rate while training\n",
    "with a large initial learning rate the model can converge fast at the begining\n",
    "with a decreasing learning rate the network can converge better at final iters\n",
    "'''\n",
    "class LearningRateScheduler():\n",
    "    def __init__(self,begin_lr,end_lr,scheduler_type):\n",
    "        self.begin_lr = begin_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.scheduler_type = scheduler_type\n",
    "        \n",
    "    def caculate(self,curr_iter,total_iter):\n",
    "        if(self.scheduler_type == 'linear'):\n",
    "            return self.__linear(curr_iter,total_iter)\n",
    "        else:\n",
    "            #program is not expected to reach here\n",
    "            assert false\n",
    "            \n",
    "    def __linear(self,curr_iter,total_iter):\n",
    "        return ((total_iter - curr_iter) * self.begin_lr + curr_iter * self.end_lr)/total_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 loss: 2.3071955731146465 learning_rate: 0.02\n",
      "iter: 50 loss: 0.8715972956564584 learning_rate: 0.019524999999999997\n",
      "iter: 100 loss: 0.3014861036324603 learning_rate: 0.01905\n",
      "iter: 150 loss: 0.19811641597449864 learning_rate: 0.018574999999999998\n",
      "iter: 200 loss: 0.17014129369546424 learning_rate: 0.0181\n",
      "iter: 250 loss: 0.16213706785458323 learning_rate: 0.017625\n",
      "iter: 300 loss: 0.16751993729225712 learning_rate: 0.01715\n",
      "iter: 350 loss: 0.13726659522999804 learning_rate: 0.016675000000000002\n",
      "iter: 400 loss: 0.12970179974529594 learning_rate: 0.0162\n",
      "iter: 450 loss: 0.07907475207672139 learning_rate: 0.015725\n",
      "iter: 500 loss: 0.07808314538949185 learning_rate: 0.01525\n",
      "iter: 550 loss: 0.08658684442810474 learning_rate: 0.014775\n",
      "iter: 600 loss: 0.10787833742941626 learning_rate: 0.0143\n",
      "iter: 650 loss: 0.07770955952734199 learning_rate: 0.013824999999999999\n",
      "iter: 700 loss: 0.09185342641594992 learning_rate: 0.013349999999999999\n",
      "iter: 750 loss: 0.05515470804732235 learning_rate: 0.012875\n",
      "iter: 800 loss: 0.05026444815665596 learning_rate: 0.0124\n",
      "iter: 850 loss: 0.05939228373773644 learning_rate: 0.011925000000000002\n",
      "iter: 900 loss: 0.07975158352962795 learning_rate: 0.01145\n",
      "iter: 950 loss: 0.05721778780351772 learning_rate: 0.010975\n",
      "iter: 1000 loss: 0.06785864320924441 learning_rate: 0.0105\n",
      "iter: 1050 loss: 0.04360226342327759 learning_rate: 0.010025000000000001\n",
      "iter: 1100 loss: 0.03697672641474793 learning_rate: 0.009550000000000001\n",
      "iter: 1150 loss: 0.04483525783375442 learning_rate: 0.009075\n",
      "iter: 1200 loss: 0.06387984960752259 learning_rate: 0.0086\n",
      "iter: 1250 loss: 0.04522276778254201 learning_rate: 0.008125\n",
      "iter: 1300 loss: 0.05294711754669758 learning_rate: 0.0076500000000000005\n",
      "iter: 1350 loss: 0.03605526292418502 learning_rate: 0.007175\n",
      "iter: 1400 loss: 0.030069381137664032 learning_rate: 0.0067\n",
      "iter: 1450 loss: 0.03777923798929872 learning_rate: 0.006225\n",
      "iter: 1500 loss: 0.05535118950205347 learning_rate: 0.00575\n",
      "iter: 1550 loss: 0.037810618137751134 learning_rate: 0.005275\n",
      "iter: 1600 loss: 0.04343663060865172 learning_rate: 0.0048\n",
      "iter: 1650 loss: 0.031462151806170825 learning_rate: 0.004325\n",
      "iter: 1700 loss: 0.026323443988436442 learning_rate: 0.00385\n",
      "iter: 1750 loss: 0.034346507123848256 learning_rate: 0.003375\n",
      "iter: 1800 loss: 0.050364642443370855 learning_rate: 0.0029\n",
      "iter: 1850 loss: 0.03357893423455107 learning_rate: 0.0024249999999999996\n",
      "iter: 1900 loss: 0.038654432846952774 learning_rate: 0.0019500000000000001\n",
      "iter: 1950 loss: 0.02953699796101593 learning_rate: 0.0014750000000000002\n",
      "iter: 2000 loss: 0.024148781163783276 learning_rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "#network defining------------------------------------------------------------\n",
    "dnn1_affine = AffineComponent(784,100)\n",
    "dnn1_relu = NolinearComponent(100,\"relu\")\n",
    "dnn2_affine = AffineComponent(100,20)\n",
    "dnn2_relu = NolinearComponent(20,\"relu\")\n",
    "dnn3_affine = AffineComponent(20,10)\n",
    "output = SoftmaxOutputComponent(10)\n",
    "\n",
    "def network_propagate(input_data):\n",
    "    activate = dnn1_affine.propagate(input_data)\n",
    "    activate = dnn1_relu.propagate(activate)\n",
    "    activate = dnn2_affine.propagate(activate)\n",
    "    activate = dnn2_relu.propagate(activate)\n",
    "    activate = dnn3_affine.propagate(activate)\n",
    "    return output.propagate(activate)\n",
    "\n",
    "def network_backpropagate(probs,batch_label):\n",
    "    derivative = output.back_propagate(probs,batch_label)\n",
    "    derivative = dnn3_affine.back_propagate(derivative)\n",
    "    derivative = dnn2_relu.back_propagate(derivative)\n",
    "    derivative = dnn2_affine.back_propagate(derivative)\n",
    "    derivative = dnn1_relu.back_propagate(derivative)\n",
    "    derivative = dnn1_affine.back_propagate(derivative)\n",
    "\n",
    "def caculate_loss(probs,batch_label):\n",
    "    batch_size = probs.shape[0]\n",
    "    loss_list = -np.log(probs[range(batch_size), batch_label])\n",
    "    average_loss = loss_list.mean(axis=0)\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "#network training------------------------------------------------------------\n",
    "#the network is overfitting without normalization, may add later\n",
    "batch_size = 1000\n",
    "total_iter = 2000\n",
    "init_lr = 0.02\n",
    "end_lr = 0.001\n",
    "lr_scheduler = LearningRateScheduler(init_lr,end_lr,'linear')\n",
    "\n",
    "for i in range(0,total_iter+1):\n",
    "    learning_rate = lr_scheduler.caculate(i,total_iter)\n",
    "    batch_data, batch_label = mnist_train.get_next_batch(batch_size)\n",
    "    probs = network_propagate(batch_data)\n",
    "    loss = caculate_loss(probs, batch_label)\n",
    "    if(i%50==0):\n",
    "        print(\"iter:\",i,\"loss:\",loss,\"learning_rate:\",learning_rate)\n",
    "    network_backpropagate(probs,batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num: 1000 correct_num: 976 total_loss: 0.07716743732753546\n",
      "total_num: 2000 correct_num: 1953 total_loss: 0.07905484055412207\n",
      "total_num: 3000 correct_num: 2931 total_loss: 0.07908418728302334\n",
      "total_num: 4000 correct_num: 3908 total_loss: 0.07822225157425092\n",
      "total_num: 5000 correct_num: 4888 total_loss: 0.07687986564714142\n",
      "total_num: 6000 correct_num: 5873 total_loss: 0.07209125418882943\n",
      "total_num: 7000 correct_num: 6846 total_loss: 0.07212297989483465\n",
      "total_num: 8000 correct_num: 7815 total_loss: 0.07781544568994414\n",
      "total_num: 9000 correct_num: 8782 total_loss: 0.08170254618881352\n",
      "total_num: 10000 correct_num: 9754 total_loss: 0.08345664912738646\n",
      "test finished, test sample:10000, accurancy:0.9754, average loss:0.08345664912738646\n"
     ]
    }
   ],
   "source": [
    "#network testing------------------------------------------------------------\n",
    "total_num = 0\n",
    "correct_num = 0\n",
    "total_loss = 0\n",
    "\n",
    "test_batch_num = 10\n",
    "test_batch_size = int(mnist_test.size / test_batch_num)\n",
    "\n",
    "mnist_test.shuffle()\n",
    "for i in range(1,test_batch_num+1):\n",
    "    #handle it in batch in case out of memory, however in this example, it's useless\n",
    "    batch_data, batch_label = mnist_test.get_next_batch(test_batch_size)\n",
    "    probs = network_propagate(batch_data)\n",
    "    prediction = probs.argmax(axis = 1).reshape(test_batch_size,1)\n",
    "    loss = caculate_loss(probs, batch_label)\n",
    "\n",
    "    total_num += test_batch_size\n",
    "    correct_num += np.sum(batch_label == prediction.T)\n",
    "    total_loss += loss\n",
    "    print('total_num:', total_num, 'correct_num:', correct_num, 'total_loss:', total_loss / i)\n",
    "\n",
    "print('test finished, test sample:{0}, accurancy:{1}, average loss:{2}'.format(total_num, float(correct_num) / total_num, total_loss / test_batch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
