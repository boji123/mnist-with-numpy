{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist data readed.\n",
      "Train set size:60000, Test set size:10000\n",
      "Data label:8\n",
      "Data size:784\n",
      "Data:\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0 115 169 253 253 253 232\n",
      "  44   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  57 241 251 252 252 252 252 252 219   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  15  94 171 240 253 252 252 252 252\n",
      " 252 252 176   0   0   0   0   0   0   0   0   0   0   0   0   0   0  99\n",
      " 213 219 252 252 252 215 116  39  65 224 252 239  63   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0 200 252 252 252 248 185  28   0  25\n",
      " 196 252 252 107   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  88 238 252 252 241  26   0  23 197 252 252 184  21   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  59 176 252 252 210  20\n",
      " 129 252 252 206  21   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  15 179 252 252 204 252 252 229  96   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  34 252\n",
      " 252 253 252 240  37   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  22 208 252 252 253 195  12   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  90 211\n",
      " 253 253 253 255 146   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  34 142 249 252 252 183 139 253 145   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  81 252\n",
      " 252 237 144  11  85 253 145   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  30 205 252 235  75   0   0 114 253 145   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 199\n",
      " 252 252  93   0   0   0 133 253 145   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0 226 252 238  52   0   0  39 221 253\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 226 252 196  41  62 173 219 252 234  52   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 226 252 252 252 252 252 252\n",
      " 252 120   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 214 250 252 252 252 252 244 168   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0 101 154 252 231\n",
      " 119  50   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "\n",
    "\n",
    "#setting logger---------------------------------------\n",
    "def init_logger():\n",
    "    logger = logging.getLogger('NNET Debug')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    # 再创建一个handler，用于输出到控制台\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    # 定义handler的输出格式\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s \\n %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(ch)\n",
    "    return logger\n",
    "#handling data---------------------------------------\n",
    "class StructData:\n",
    "    def __init__(self,data_set):\n",
    "        self.size = len(data_set[0])\n",
    "        self.data = np.array(data_set[0])\n",
    "        self.label = np.array(data_set[1])\n",
    "        self.curr_batch_start = 0;\n",
    "        \n",
    "    def shuffle(self):\n",
    "        append_list = np.append(self.data, self.label.reshape(self.size,1), axis = 1)\n",
    "        np.random.shuffle(append_list)\n",
    "        self.data = append_list[:,:-1]\n",
    "        self.label = append_list[:,-1]\n",
    "    #act like a circular list\n",
    "    def get_next_batch(self,batch_size):\n",
    "        start = self.curr_batch_start\n",
    "        end = (self.curr_batch_start + batch_size) % self.size \n",
    "        self.curr_batch_start = end\n",
    "\n",
    "        if(start < end):\n",
    "            return self.data[start:end],self.label[start:end]\n",
    "        else:\n",
    "            data = np.append(self.data[start:self.size], self.data[0:end], axis = 0)\n",
    "            label = np.append(self.label[start:self.size], self.label[0:end], axis = 0)\n",
    "            return data,label\n",
    "#reading data---------------------------------------        \n",
    "mndata = MNIST('mnist_data')\n",
    "mnist_train=StructData(mndata.load_training())\n",
    "mnist_train.shuffle()\n",
    "mnist_test=StructData(mndata.load_testing())\n",
    "#---------------------------------------\n",
    "'''\n",
    "logger = init_logger()\n",
    "logger.info('Mnist data readed.')\n",
    "logger.info('Train set size:{0},Test set size:{1}'.format(len(mnist_train.data),len(mnist_test.data)))\n",
    "logger.debug('Data size:{0}'.format(len(mnist_test.data[0])))\n",
    "logger.debug('Data:\\n{0}'.format(mnist_test.data[0]))\n",
    "logger.debug('Data label:{0}'.format(mnist_test.label[0]))\n",
    "'''\n",
    "#---------------------------------------\n",
    "print('Mnist data readed.')\n",
    "print('Train set size:{0}, Test set size:{1}'.format(len(mnist_train.data),len(mnist_test.data)))\n",
    "print('Data label:{0}'.format(mnist_train.label[0]))\n",
    "print('Data size:{0}'.format(len(mnist_train.data[0])))\n",
    "print('Data:\\n{0}'.format(mnist_train.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network component defining---------------------------------\n",
    "class AffineComponent:\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = 0.01 * np.random.randn(input_dim, output_dim)\n",
    "        self.bias = 0.01 * np.random.randn(1,output_dim)\n",
    "\n",
    "    def propagate(self,input_data):\n",
    "        assert input_data.shape[1] == self.input_dim\n",
    "        self.input_data = input_data\n",
    "        return input_data.dot(self.weights) + self.bias\n",
    "\n",
    "    def back_propagate(self,derivative):\n",
    "        assert derivative.shape[1] == self.output_dim\n",
    "        propagate_derivative = derivative.dot(self.weights.T)\n",
    "        self.__update(derivative)\n",
    "        return propagate_derivative\n",
    "\n",
    "    def __update(self,derivative):\n",
    "        #the learning rate is a global value which will change while training\n",
    "        #see more detain in LearningRateScheduler\n",
    "        self.bias -= learning_rate * derivative.sum(axis = 0, keepdims= True )\n",
    "        self.weights -= learning_rate * self.input_data.T.dot(derivative)\n",
    "\n",
    "\n",
    "class NolinearComponent:\n",
    "    def __init__(self,dim,nolinear_type):\n",
    "        self.dim = dim\n",
    "        self.nolinear_type = nolinear_type\n",
    "\n",
    "    def propagate(self,input_data):\n",
    "        assert input_data.shape[1] == self.dim\n",
    "        self.input_data = input_data\n",
    "        if(self.nolinear_type == \"relu\"):\n",
    "            return self.__relu(input_data)\n",
    "        else:\n",
    "            #program is not expected to reach here\n",
    "            assert false\n",
    "\n",
    "    def __relu(self,input_data):\n",
    "        #important! must use copy or the input data will be change through index\n",
    "        output_data = input_data.copy()\n",
    "        output_data[output_data < 0] = 0\n",
    "        return output_data\n",
    "    #----------------------------\n",
    "    def back_propagate(self,derivative):\n",
    "        assert derivative.shape[1] == self.dim\n",
    "        if(self.nolinear_type == \"relu\"):\n",
    "            return self.__back_relu(derivative)\n",
    "        else:\n",
    "            #program is not expected to reach here\n",
    "            assert false\n",
    "            \n",
    "    def __back_relu(self,derivative):\n",
    "        derivative[self.input_data < 0] = 0\n",
    "        return derivative\n",
    "\n",
    "\n",
    "class SoftmaxOutputComponent:\n",
    "    def __init__(self,dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def propagate(self,input_data):\n",
    "        assert input_data.shape[1] == self.dim\n",
    "        self.input_data = input_data\n",
    "        e_x = np.exp(input_data)\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def back_propagate(self,probs,label):\n",
    "        assert probs.shape[0] == label.shape[0]\n",
    "        batch_size = probs.shape[0]\n",
    "        delta = probs\n",
    "        delta[range(batch_size),batch_label] -= 1\n",
    "        return delta / batch_size\n",
    "\n",
    "'''\n",
    "this class aims to change the learning rate while training\n",
    "with a large initial learning rate the model can converge fast at the begining\n",
    "with a decreasing learning rate the network can converge better at final iters\n",
    "'''\n",
    "class LearningRateScheduler():\n",
    "    def __init__(self,begin_lr,end_lr,scheduler_type):\n",
    "        self.begin_lr = begin_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.scheduler_type = scheduler_type\n",
    "        \n",
    "    def caculate(self,curr_iter,total_iter):\n",
    "        if(self.scheduler_type == 'linear'):\n",
    "            return self.__linear(curr_iter,total_iter)\n",
    "        else:\n",
    "            #program is not expected to reach here\n",
    "            assert false\n",
    "            \n",
    "    def __linear(self,curr_iter,total_iter):\n",
    "        return ((total_iter - curr_iter) * self.begin_lr + curr_iter * self.end_lr)/total_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 loss: 2.3079089004432207 learning_rate: 0.02\n",
      "iter: 50 loss: 0.6914574453684411 learning_rate: 0.019524999999999997\n",
      "iter: 100 loss: 0.38315983009164784 learning_rate: 0.01905\n",
      "iter: 150 loss: 0.2618309414502422 learning_rate: 0.018574999999999998\n",
      "iter: 200 loss: 0.19331335423116147 learning_rate: 0.0181\n",
      "iter: 250 loss: 0.14395422973094862 learning_rate: 0.017625\n",
      "iter: 300 loss: 0.13023613769550316 learning_rate: 0.01715\n",
      "iter: 350 loss: 0.14283629196981854 learning_rate: 0.016675000000000002\n",
      "iter: 400 loss: 0.11829808621121171 learning_rate: 0.0162\n",
      "iter: 450 loss: 0.09812335360357607 learning_rate: 0.015725\n",
      "iter: 500 loss: 0.11195963760305722 learning_rate: 0.01525\n",
      "iter: 550 loss: 0.07757882389254178 learning_rate: 0.014775\n",
      "iter: 600 loss: 0.0754515288020256 learning_rate: 0.0143\n",
      "iter: 650 loss: 0.0802762196920673 learning_rate: 0.013824999999999999\n",
      "iter: 700 loss: 0.07090762232024998 learning_rate: 0.013349999999999999\n",
      "iter: 750 loss: 0.06489502930908896 learning_rate: 0.012875\n",
      "iter: 800 loss: 0.07351338928299293 learning_rate: 0.0124\n",
      "iter: 850 loss: 0.05279262455650193 learning_rate: 0.011925000000000002\n",
      "iter: 900 loss: 0.056248735543133664 learning_rate: 0.01145\n",
      "iter: 950 loss: 0.05369835368123181 learning_rate: 0.010975\n",
      "iter: 1000 loss: 0.052471081553977776 learning_rate: 0.0105\n",
      "iter: 1050 loss: 0.048923044578479985 learning_rate: 0.010025000000000001\n",
      "iter: 1100 loss: 0.05418303383034492 learning_rate: 0.009550000000000001\n",
      "iter: 1150 loss: 0.04092665450302671 learning_rate: 0.009075\n",
      "iter: 1200 loss: 0.045886863389511416 learning_rate: 0.0086\n",
      "iter: 1250 loss: 0.03960888555582945 learning_rate: 0.008125\n",
      "iter: 1300 loss: 0.04095258066168115 learning_rate: 0.0076500000000000005\n",
      "iter: 1350 loss: 0.03887886170174387 learning_rate: 0.007175\n",
      "iter: 1400 loss: 0.042808114591635 learning_rate: 0.0067\n",
      "iter: 1450 loss: 0.03435886092050339 learning_rate: 0.006225\n",
      "iter: 1500 loss: 0.038900227429068464 learning_rate: 0.00575\n",
      "iter: 1550 loss: 0.031911531208554896 learning_rate: 0.005275\n",
      "iter: 1600 loss: 0.03345682974724741 learning_rate: 0.0048\n",
      "iter: 1650 loss: 0.033010655050872466 learning_rate: 0.004325\n",
      "iter: 1700 loss: 0.03651913973531053 learning_rate: 0.00385\n",
      "iter: 1750 loss: 0.030899008684460307 learning_rate: 0.003375\n",
      "iter: 1800 loss: 0.03427586879613543 learning_rate: 0.0029\n",
      "iter: 1850 loss: 0.027652894890225682 learning_rate: 0.0024249999999999996\n"
     ]
    }
   ],
   "source": [
    "#network defining------------------------------------------------------------\n",
    "dnn1_affine = AffineComponent(784,100)\n",
    "dnn1_relu = NolinearComponent(100,\"relu\")\n",
    "dnn2_affine = AffineComponent(100,20)\n",
    "dnn2_relu = NolinearComponent(20,\"relu\")\n",
    "dnn3_affine = AffineComponent(20,10)\n",
    "output = SoftmaxOutputComponent(10)\n",
    "\n",
    "def network_propagate(input_data):\n",
    "    activate = dnn1_affine.propagate(input_data)\n",
    "    activate = dnn1_relu.propagate(activate)\n",
    "    activate = dnn2_affine.propagate(activate)\n",
    "    activate = dnn2_relu.propagate(activate)\n",
    "    activate = dnn3_affine.propagate(activate)\n",
    "    return output.propagate(activate)\n",
    "\n",
    "def network_backpropagate(probs,batch_label):\n",
    "    derivative = output.back_propagate(probs,batch_label)\n",
    "    derivative = dnn3_affine.back_propagate(derivative)\n",
    "    derivative = dnn2_relu.back_propagate(derivative)\n",
    "    derivative = dnn2_affine.back_propagate(derivative)\n",
    "    derivative = dnn1_relu.back_propagate(derivative)\n",
    "    derivative = dnn1_affine.back_propagate(derivative)\n",
    "\n",
    "def caculate_loss(probs,batch_label):\n",
    "    batch_size = probs.shape[0]\n",
    "    loss_list = -np.log(probs[range(batch_size), batch_label])\n",
    "    average_loss = loss_list.mean(axis=0)\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "#network training------------------------------------------------------------\n",
    "#the network is overfitting without normalization, may add later\n",
    "batch_size = 1000\n",
    "total_iter = 2000\n",
    "init_lr = 0.02\n",
    "end_lr = 0.001\n",
    "lr_scheduler = LearningRateScheduler(init_lr,end_lr,'linear')\n",
    "\n",
    "for i in range(0,total_iter+1):\n",
    "    learning_rate = lr_scheduler.caculate(i,total_iter)\n",
    "    batch_data, batch_label = mnist_train.get_next_batch(batch_size)\n",
    "    probs = network_propagate(batch_data)\n",
    "    loss = caculate_loss(probs, batch_label)\n",
    "    if(i%50==0):\n",
    "        print(\"iter:\",i,\"loss:\",loss,\"learning_rate:\",learning_rate)\n",
    "    network_backpropagate(probs,batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0 1000\n",
      "total_num: 1000 correct_num: 972 total_loss: 0.11013611048982141\n",
      "1000 2000\n",
      "total_num: 2000 correct_num: 1953 total_loss: 0.08086434463241975\n",
      "2000 3000\n",
      "total_num: 3000 correct_num: 2929 total_loss: 0.08081760637388924\n",
      "3000 4000\n",
      "total_num: 4000 correct_num: 3911 total_loss: 0.08042349699258702\n",
      "4000 5000\n",
      "total_num: 5000 correct_num: 4884 total_loss: 0.082576614713133\n",
      "5000 6000\n",
      "total_num: 6000 correct_num: 5856 total_loss: 0.08257731643183631\n",
      "6000 7000\n",
      "total_num: 7000 correct_num: 6828 total_loss: 0.0829946193063779\n",
      "7000 8000\n",
      "total_num: 8000 correct_num: 7804 total_loss: 0.08061730248922405\n",
      "8000 9000\n",
      "total_num: 9000 correct_num: 8772 total_loss: 0.08127990722465538\n",
      "9000 0\n",
      "total_num: 10000 correct_num: 9738 total_loss: 0.08382750297504701\n",
      "test finished, test sample:10000, accurancy:0.9738, average loss:0.08382750297504701\n"
     ]
    }
   ],
   "source": [
    "#network testing------------------------------------------------------------\n",
    "total_num = 0\n",
    "correct_num = 0\n",
    "total_loss = 0\n",
    "\n",
    "test_batch_num = 10\n",
    "test_batch_size = int(mnist_test.size / test_batch_num)\n",
    "print(test_batch_size)\n",
    "mnist_test.shuffle()\n",
    "for i in range(1,test_batch_num+1):\n",
    "    #handle it in batch in case out of memory, however in this example, it's useless\n",
    "    batch_data, batch_label = mnist_test.get_next_batch(test_batch_size)\n",
    "    probs = network_propagate(batch_data)\n",
    "    prediction = probs.argmax(axis = 1).reshape(test_batch_size,1)\n",
    "    loss = caculate_loss(probs, batch_label)\n",
    "\n",
    "    total_num += test_batch_size\n",
    "    correct_num += np.sum(batch_label == prediction.T)\n",
    "    total_loss += loss\n",
    "    print('total_num:', total_num, 'correct_num:', correct_num, 'total_loss:', total_loss / i)\n",
    "\n",
    "print('test finished, test sample:{0}, accurancy:{1}, average loss:{2}'.format(total_num, float(correct_num) / total_num, total_loss / test_batch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
